{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gfelekis/MSc-Dissertation/blob/master/Ensemble_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "1QS81BeEg6Tq",
        "colab": {}
      },
      "source": [
        "#@title Imports\n",
        "!pip install GPy\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.stats\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import os\n",
        "import GPy\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbz3rFUlEUrG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title UCI Datasets\n",
        "#Boston housing dataset\n",
        "np.random.seed(2)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\" --no-check-certificate \n",
        "data1 = pd.read_csv('housing.data', header=0, delimiter=\"\\s+\").values\n",
        "data1 = data1[np.random.permutation(np.arange(len(data1)))]\n",
        "\n",
        "# Concrete compressive dataset\n",
        "np.random.seed(2)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\" --no-check-certificate\n",
        "data2 = pd.read_excel('Concrete_Data.xls', header=0, delimiter=\"\\s+\").values\n",
        "data2 = data2[np.random.permutation(np.arange(len(data2)))]\n",
        "\n",
        "# Energy efficiency dataset\n",
        "np.random.seed(2)\n",
        "!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\" --no-check-certificate\n",
        "data3 = pd.read_excel('ENB2012_data.xlsx', header=0, delimiter=\"\\s+\").values\n",
        "data3 = data3[np.random.permutation(np.arange(len(data3)))]\n",
        "\n",
        "# Red wine dataset\n",
        "np.random.seed(2)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\" --no-check-certificate \n",
        "data4 = pd.read_csv('winequality-red.csv', header=1, delimiter=';').values\n",
        "data4 = data4[np.random.permutation(np.arange(len(data4)))]\n",
        "\n",
        "#Yacht dataset\n",
        "np.random.seed(2)\n",
        "!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\" --no-check-certificate \n",
        "data5 = pd.read_csv('yacht_hydrodynamics.data', header=1, delimiter='\\s+').values\n",
        "data5 = data5[np.random.permutation(np.arange(len(data5)))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGqAWMNFhi78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.manual_seed_all(999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1S5kt0omQ-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va8V78eFFsc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma)\n",
        "    \n",
        "    return - (log_coeff + exponent).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQFXw0se_4bE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Ensemble_UCI(torch.nn.Module):\n",
        "    def __init__(self, n_feature, num_units, n_output, learn_rate, weight_decay):\n",
        "        super(Ensemble_UCI, self).__init__()\n",
        "        torch.manual_seed(42)\n",
        "        input_dim = n_feature\n",
        "        output_dim = n_output\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.num_units = num_units\n",
        "      \n",
        "        # network with two hidden and one output layer\n",
        "        if len(num_units) == 1:\n",
        "          self.layer1 = nn.Linear(input_dim, num_units[0])\n",
        "          self.layer2 = nn.Linear(num_units[0], 2*output_dim)\n",
        "        if len(num_units) == 2:\n",
        "          self.layer1 = nn.Linear(input_dim, num_units[0])\n",
        "          self.layer2 = nn.Linear(num_units[0], num_units[1])\n",
        "          self.layer3 = nn.Linear(num_units[1], 2*output_dim)\n",
        "        elif len(num_units) == 3:\n",
        "          self.layer1 = nn.Linear(input_dim, num_units[0])\n",
        "          self.layer2 = nn.Linear(num_units[0], num_units[1])\n",
        "          self.layer3 = nn.Linear(num_units[1], num_units[2])\n",
        "          self.layer4 = nn.Linear(num_units[2], 2*output_dim)\n",
        "\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "        \n",
        "        self.loss_func = log_gaussian_loss\n",
        "        self.optimizer = torch.optim.SGD(self.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if len(self.num_units) == 1:\n",
        "          x = self.layer1(x)\n",
        "          x = self.activation(x)\n",
        "          x = self.layer2(x)\n",
        "\n",
        "        if len(self.num_units) == 2:\n",
        "          x = self.layer1(x)\n",
        "          x = self.activation(x)\n",
        "\n",
        "          x = self.layer2(x)\n",
        "          x = self.activation(x)\n",
        "\n",
        "          x = self.layer3(x)\n",
        "\n",
        "        elif len(self.num_units) == 3:\n",
        "          x = self.layer1(x)\n",
        "          x = self.activation(x)\n",
        "\n",
        "          x = self.layer2(x)\n",
        "          x = self.activation(x)\n",
        "\n",
        "          x = self.layer3(x)\n",
        "          x = self.activation(x)\n",
        "\n",
        "          x = self.layer4(x)\n",
        "                \n",
        "        return x\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        torch.manual_seed(42)\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.forward(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)/x.shape[0]\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J_HMo9VJd5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_ensemble(x, y, ensemble):\n",
        "    torch.manual_seed(42)\n",
        "    \n",
        "    x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "    means, stds = [], []\n",
        "    for net in ensemble:\n",
        "        output = net(x)\n",
        "        means.append(output[:, :1, None])\n",
        "        stds.append(output[:, 1:, None].exp())\n",
        "            \n",
        "    means, stds = torch.cat(means, 2), torch.cat(stds, 2)\n",
        "    mean = means.mean(dim=2)\n",
        "    std = (means.var(dim=2) + stds.mean(dim=2)**2)**0.5\n",
        "    loss = log_gaussian_loss(mean, y, std, 1)/len(x)\n",
        "    \n",
        "    rmse = ((mean - y)**2).mean()**0.5\n",
        "\n",
        "    return loss, rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0gfEDv6KgSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_ensemble(data, n_splits, num_epochs, num_nets, num_hidden, learn_rate, weight_decay, data_fraction, log_every):\n",
        "    torch.manual_seed(42)\n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    in_dim = data.shape[1] - 1\n",
        "    train_logliks, test_logliks = [], []\n",
        "    train_rmses, test_rmses = [], []\n",
        "\n",
        "    for j, idx in enumerate(kf.split(data)):\n",
        "        train_index, test_index = idx\n",
        "\n",
        "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
        "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
        "\n",
        "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
        "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
        "\n",
        "        x_train = (x_train - x_means)/x_stds\n",
        "        y_train = (y_train - y_means)/y_stds\n",
        "\n",
        "        x_test = (x_test - x_means)/x_stds\n",
        "        y_test = (y_test - y_means)/y_stds\n",
        "\n",
        "        batch_size = len(x_train) \n",
        "\n",
        "        fit_loss_train = np.zeros(num_epochs)\n",
        "        best_net, best_loss = None, float('inf')\n",
        "        nets = []\n",
        "\n",
        "        for n in range(num_nets):\n",
        "            net = Ensemble_UCI(n_feature=in_dim, num_units=num_hidden, n_output=1, learn_rate=learn_rate, weight_decay=weight_decay).cuda()\n",
        "\n",
        "            sub_idx = np.random.choice(np.arange(0, len(x_train)), size = (int(len(x_train)*data_fraction),), replace=True)\n",
        "            x_train_sub, y_train_sub = x_train[sub_idx], y_train[sub_idx]\n",
        "\n",
        "            for i in range(num_epochs):\n",
        "\n",
        "                loss = net.fit(x_train_sub, y_train_sub)\n",
        "\n",
        "                if log_every is not False and (i % log_every == 0 or i == num_epochs - 1) and len(nets) > 0:\n",
        "                    train_loss, train_rmse = eval_ensemble(x_train, y_train, nets)\n",
        "                    test_loss, test_rmse = eval_ensemble(x_test, y_test, nets)\n",
        "                    print('Epoch %3d, network %2d, Loss train/test %.3f/%.3f, RMSE train/test %.3f/%.3f' % \\\n",
        "                          (i+1, len(nets), train_loss.cpu().data.numpy(), test_loss.cpu().data.numpy(),\n",
        "                          train_rmse.cpu().data.numpy(), test_rmse.cpu().data.numpy()))\n",
        "\n",
        "            nets.append(copy.deepcopy(net))\n",
        "\n",
        "\n",
        "        train_loss, train_rmse = eval_ensemble(x_train, y_train, nets)\n",
        "        test_loss, test_rmse = eval_ensemble(x_test, y_test, nets)\n",
        "\n",
        "        train_logliks.append(-train_loss.cpu().data.numpy() - np.log(y_stds)[0])\n",
        "        test_logliks.append(-test_loss.cpu().data.numpy() - np.log(y_stds)[0])\n",
        "\n",
        "        train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
        "        test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
        "\n",
        "    print('Train log. lik. = %7.3f +/- %.3f' % (np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
        "    print('Test  log. lik. = %7.3f +/- %.3f' % (np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
        "    print('Train RMSE      = %7.3f +/- %.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
        "    print('Test  RMSE      = %7.3f +/- %.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
        "    \n",
        "    rmses =  list(np.array(test_rmses).flatten())\n",
        "    print(\"Test LogLike for different folds: \", test_logliks)\n",
        "    print(\"Test RMSEs   for different folds: \", rmses)\n",
        "\n",
        "    metrics = {\"train_log_like_mean\": -np.array(train_logliks).mean(), \"train_log_like_var\": np.array(train_logliks).var()**0.5,\n",
        "               \"test_log_like_mean\": -np.array(test_logliks).mean(), \"test_log_like_var\": np.array(test_logliks).var()**0.5,\n",
        "               \"train_rmse_mean\": np.array(train_rmses).mean(), \"train_rmse_var\": np.array(train_rmses).var()**0.5,\n",
        "               \"test_rmse_mean\": np.array(test_rmses).mean(), \"test_rmse_var\":np.array(test_rmses).var()**0.5,\n",
        "               \"rmse_values\": list(np.array(test_rmses).flatten()),\n",
        "               \"loglik_values\": list(np.array(test_logliks).flatten()),\n",
        "               }\n",
        "\n",
        "    return nets, metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtjriqWwRfR9",
        "colab_type": "text"
      },
      "source": [
        "### RUN THE EXPERIMENTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkO1JjQh6gW_",
        "colab_type": "text"
      },
      "source": [
        "# Regression on UCI data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuQCoQx7RheW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be32fd0d-5426-4820-f954-3cc2cfac2ce1"
      },
      "source": [
        "# set up the access to drive - we'll be saving our logs there\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "# define paths for the different experiments\n",
        "ensemble_path = \"/content/drive/My Drive/\"+\"new_ensemble_logs.txt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heLvvZiMRrgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logs = []\n",
        "### some params\n",
        "n_splits = 30\n",
        "n_epochs = 100\n",
        "hidden   = 100\n",
        "models, log_metrics = [], []\n",
        "\n",
        "### alphas_experiment\n",
        "dataset = [ data1, data2, data3, data4, data5] # list of all the datasets\n",
        "dataset_names = [\"Boston\",\"Concrete\", \"Energy\", \"Wine\", \"Yacht\"]\n",
        "hiddens = [1, 2, 3]\n",
        "\n",
        "for i, data in enumerate(dataset):\n",
        "  dataset_name = dataset_names[i]\n",
        "  for h in hiddens:\n",
        "    # run the training\n",
        "    num_units = [hidden for i in range(h)]\n",
        "    model, metric  = train_ensemble(data=data, n_splits=n_splits, num_epochs=n_epochs,\n",
        "                                    num_nets=20, num_hidden=num_units, learn_rate=1e-2,\n",
        "                                    weight_decay=1e-2, data_fraction=0.8, log_every=False)\n",
        "    models.append(model)\n",
        "    # record to file: \n",
        "    log = {\"dataset\": dataset_name,\n",
        "           \"loss\": \"ensemble\",\n",
        "           \"alpha\": \"n_nets_\"+str(20),\n",
        "           \"n_layers\": h, \n",
        "           \"constant_hidden_size\": hidden, \n",
        "           \"metrics\": metric}\n",
        "    logs.append(log)\n",
        "    with open(ensemble_path, \"a\") as f:\n",
        "      f.write(str(log)+\"\\n\")\n",
        "      print(log)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw95bt9l6jdT",
        "colab_type": "text"
      },
      "source": [
        "# Regression on GP ground truth\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14Zvx-vNrQcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_uncertainty_ens (nets, x_train, y_train):\n",
        "  values, noises = [], []\n",
        "  for net in nets:\n",
        "      preds = net(torch.linspace(-5, 5, 200)[:, None].cuda())\n",
        "      values.append(preds[:, 0].cpu().data.numpy())\n",
        "      noises.append(preds[:, 1].exp().cpu().data.numpy())\n",
        "        \n",
        "  values = np.array(values).reshape(num_nets, 200)\n",
        "  means, epistemic = values.mean(axis = 0), values.var(axis = 0)**0.5\n",
        "  noises = np.array(noises)\n",
        "  aleatoric = (noises**2).mean(axis = 0)**0.5\n",
        "\n",
        "  total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
        "\n",
        "  c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
        "      '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
        "\n",
        "  plt.figure(figsize = (6, 5))\n",
        "  plt.style.use('default')\n",
        "  plt.scatter(x_train, y_train, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n",
        "  plt.fill_between(np.linspace(-5, 5, 200), means + aleatoric, means + total_unc, color = c[0], alpha = 0.3)\n",
        "  plt.fill_between(np.linspace(-5, 5, 200), means - total_unc, means - aleatoric, color = c[0], alpha = 0.3)\n",
        "  plt.fill_between(np.linspace(-5, 5, 200), means - aleatoric, means + aleatoric, color = c[4], alpha = 0.4)\n",
        "  plt.plot(np.linspace(-5, 5, 200), means, color = 'black', linewidth = 1)\n",
        "  plt.xlim([-5, 5])\n",
        "  plt.ylim([-5, 7])\n",
        "  plt.xlabel('$x$', fontsize=10)\n",
        "  plt.title('MAP Ensemble', fontsize=10)\n",
        "  plt.tick_params(labelsize=10)\n",
        "  plt.xticks(np.arange(-4, 5, 2))\n",
        "  plt.yticks(np.arange(-4, 7, 2))\n",
        "  plt.gca().yaxis.grid(alpha=0.3)\n",
        "  plt.gca().xaxis.grid(alpha=0.3)\n",
        "  plt.savefig('map_hetero.pdf', bbox_inches = 'tight')\n",
        "\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfakqEXxZ494",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_uncertainty_3row_ens(h_nets, x, y):\n",
        "  fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
        "  fig.suptitle(\"Ensemble h=1 | h=2 | h=3\")\n",
        "  for n, nets in enumerate(h_nets):\n",
        "    values, noises = [], []\n",
        "    for net in nets:\n",
        "        preds = net(torch.linspace(-5, 5, 200)[:, None].cuda())\n",
        "        values.append(preds[:, 0].cpu().data.numpy())\n",
        "        noises.append(preds[:, 1].exp().cpu().data.numpy())\n",
        "          \n",
        "    values = np.array(values).reshape(num_nets, 200)\n",
        "    means, epistemic = values.mean(axis = 0), values.var(axis = 0)**0.5\n",
        "    noises = np.array(noises)\n",
        "    aleatoric = (noises**2).mean(axis = 0)**0.5\n",
        "\n",
        "    total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
        "\n",
        "    c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
        "        '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
        "\n",
        "    ax[n].scatter(x, y, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n",
        "    ax[n].fill_between(np.linspace(-5, 5, 200), means + aleatoric, means + total_unc, color = c[0], alpha = 0.3, label = 'Epistemic + Aleatoric')\n",
        "    ax[n].fill_between(np.linspace(-5, 5, 200), means - total_unc, means - aleatoric, color = c[0], alpha = 0.3)\n",
        "    ax[n].fill_between(np.linspace(-5, 5, 200), means - aleatoric, means + aleatoric, color = c[4], alpha = 0.4, label = 'Aleatoric')\n",
        "    ax[n].plot(np.linspace(-5, 5, 200), means, color = 'black', linewidth = 1)\n",
        "    ax[n].set_xlim([-5, 5])\n",
        "    ax[n].set_ylim([-5, 7])\n",
        "    ax[n].set_xlabel('$x$', fontsize=10)\n",
        "    ax[n].set_title(\"h = \"+str(n+1), fontsize=10)\n",
        "    ax[n].tick_params(labelsize=10)\n",
        "    ax[n].set_xticks(np.arange(-4, 5, 2))\n",
        "    ax[n].set_yticks(np.arange(-4, 7, 2))\n",
        "    plt.gca().set_yticklabels([])\n",
        "    ax[n].grid(alpha=0.3)\n",
        "\n",
        "  plt.savefig('ensemble_hetero.pdf', bbox_inches = 'tight')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym6HBK-s8GnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2)\n",
        "no_points = 400\n",
        "lengthscale = 1\n",
        "variance = 1.0\n",
        "sig_noise = 0.3\n",
        "x = np.random.uniform(-3, 3, no_points)[:, None]\n",
        "x.sort(axis = 0)\n",
        "\n",
        "\n",
        "k = GPy.kern.RBF(input_dim = 1, variance=variance, lengthscale=lengthscale)\n",
        "C = k.K(x, x) + np.eye(no_points)*(x + 2)**2*sig_noise**2\n",
        "\n",
        "y = np.random.multivariate_normal(np.zeros((no_points)), C)[:, None]\n",
        "y = (y - y.mean())\n",
        "x_train = x[75:325]\n",
        "y_train = y[75:325]\n",
        "\n",
        "# h_nets = []\n",
        "nb_epochs, batch_size = 1000, len(x_train)\n",
        "for num_units in [[200], [200, 300], [200, 300, 200]]:\n",
        "  fit_loss_train = np.zeros(nb_epochs)\n",
        "  best_net, best_loss = None, float('inf')\n",
        "  num_nets, nets, losses = 20, [], []\n",
        "\n",
        "  for n in range(num_nets): # \n",
        "      net = Ensemble_UCI(n_feature=1, num_units=num_units, n_output=1, learn_rate=1e-2, weight_decay=1e-1).cuda()\n",
        "      \n",
        "      sub_idx = np.random.choice(np.arange(0, len(x_train)), size = (int(len(x_train)*0.5),), replace=True)\n",
        "      x_train_sub, y_train_sub = x_train[sub_idx], y_train[sub_idx]\n",
        "      \n",
        "      for i in range(nb_epochs): # nb_epochs\n",
        "\n",
        "          loss = net.fit(x_train_sub, y_train_sub)\n",
        "\n",
        "          if i % 500 == 0:\n",
        "              print('Network %2d, Epoch %4d, Train loss = %6.3f' % (len(nets)+1, i, loss.cpu().data.numpy()))\n",
        "              \n",
        "      nets.append(copy.deepcopy(net))\n",
        "  #h_nets.append(copy.copy(nets))\n",
        "  plot_uncertainty_ens(nets, x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3oYwCIJ5shL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}