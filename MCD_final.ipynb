{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MCD_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gfelekis/MSc-Dissertation/blob/master/MCD_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "cellView": "form",
        "id": "1QS81BeEg6Tq",
        "colab": {}
      },
      "source": [
        "#@title Imports\n",
        "!pip install GPy\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.stats\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import os\n",
        "import GPy\n",
        "import time\n",
        "import copy\n",
        "import math\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.sgd import SGD\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from google.colab import files\n",
        "%config InlineBackend.figure_format = 'svg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbz3rFUlEUrG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title UCI Datasets\n",
        "#Boston housing dataset\n",
        "np.random.seed(2)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\" --no-check-certificate \n",
        "data1 = pd.read_csv('housing.data', header=0, delimiter=\"\\s+\").values\n",
        "data1 = data1[np.random.permutation(np.arange(len(data1)))]\n",
        "\n",
        "# Concrete compressive dataset\n",
        "np.random.seed(2)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls\" --no-check-certificate\n",
        "data2 = pd.read_excel('Concrete_Data.xls', header=0, delimiter=\"\\s+\").values\n",
        "data2 = data2[np.random.permutation(np.arange(len(data2)))]\n",
        "\n",
        "# Energy efficiency dataset\n",
        "np.random.seed(2)\n",
        "!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\" --no-check-certificate\n",
        "data3 = pd.read_excel('ENB2012_data.xlsx', header=0, delimiter=\"\\s+\").values\n",
        "data3 = data3[np.random.permutation(np.arange(len(data3)))]\n",
        "\n",
        "# Red wine dataset\n",
        "np.random.seed(2)\n",
        "!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\" --no-check-certificate \n",
        "data4 = pd.read_csv('winequality-red.csv', header=1, delimiter=';').values\n",
        "data4 = data4[np.random.permutation(np.arange(len(data4)))]\n",
        "\n",
        "#Yacht dataset\n",
        "np.random.seed(2)\n",
        "!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/00243/yacht_hydrodynamics.data\" --no-check-certificate \n",
        "data5 = pd.read_csv('yacht_hydrodynamics.data', header=1, delimiter='\\s+').values\n",
        "data5 = data5[np.random.permutation(np.arange(len(data5)))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGqAWMNFhi78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.manual_seed_all(999)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNSdSVnSorhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_variable(var=(), cuda=True, volatile=False):\n",
        "    out = []\n",
        "    for v in var:\n",
        "        \n",
        "        if isinstance(v, np.ndarray):\n",
        "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
        "\n",
        "        if not v.is_cuda and cuda:\n",
        "            v = v.cuda()\n",
        "\n",
        "        if not isinstance(v, Variable):\n",
        "            v = Variable(v, volatile=volatile)\n",
        "\n",
        "        out.append(v)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovd_IWAepALw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_gaussian_loss(output, target, sigma, no_dim):\n",
        "    exponent = -0.5*(target - output)**2/sigma**2\n",
        "    log_coeff = -no_dim*torch.log(sigma) - 0.5*no_dim*np.log(2*np.pi)\n",
        "    \n",
        "    return - (log_coeff + exponent).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr_H64y4pdWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MC_Dropout_Model_UCI(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, num_units, drop_prob):\n",
        "        super(MC_Dropout_Model_UCI, self).__init__()\n",
        "        #torch.manual_seed(42)\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.drop_prob = drop_prob\n",
        "        self.num_units = num_units\n",
        "\n",
        "        # network with two hidden and one output layer\n",
        "        if len(num_units) == 1:\n",
        "          self.layer1 = nn.Linear(input_dim, num_units[0])\n",
        "          self.layer2 = nn.Linear(num_units[0], 2*output_dim)\n",
        "        if len(num_units) == 2:\n",
        "          self.layer1 = nn.Linear(input_dim, num_units[0])\n",
        "          self.layer2 = nn.Linear(num_units[0], num_units[1])\n",
        "          self.layer3 = nn.Linear(num_units[1], 2*output_dim)\n",
        "        elif len(num_units) == 3:\n",
        "          self.layer1 = nn.Linear(input_dim, num_units[0])\n",
        "          self.layer2 = nn.Linear(num_units[0], num_units[1])\n",
        "          self.layer3 = nn.Linear(num_units[1], num_units[2])\n",
        "          self.layer4 = nn.Linear(num_units[2], 2*output_dim)\n",
        "\n",
        "        self.activation = nn.ReLU(inplace = True)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_dim)\n",
        "\n",
        "        if len(self.num_units) == 1:\n",
        "          x = self.layer1(x)\n",
        "          x = self.activation(x)\n",
        "          x = F.dropout(x, p=self.drop_prob, training=True)\n",
        "          x = self.layer2(x)\n",
        "\n",
        "        if len(self.num_units) == 2:\n",
        "          x = self.layer1(x)\n",
        "          x = self.activation(x)\n",
        "          x = F.dropout(x, p=self.drop_prob, training=True)\n",
        "\n",
        "          x = self.layer2(x)\n",
        "          x = self.activation(x)\n",
        "          x = F.dropout(x, p=self.drop_prob, training=True)\n",
        "\n",
        "          x = self.layer3(x)\n",
        "\n",
        "        elif len(self.num_units) == 3:\n",
        "          x = self.layer1(x)\n",
        "          x = self.activation(x)\n",
        "          x = F.dropout(x, p=self.drop_prob, training=True)\n",
        "\n",
        "          x = self.layer2(x)\n",
        "          x = self.activation(x)\n",
        "          x = F.dropout(x, p=self.drop_prob, training=True)\n",
        "\n",
        "          x = self.layer3(x)\n",
        "          x = self.activation(x)\n",
        "          x = F.dropout(x, p=self.drop_prob, training=True)\n",
        "\n",
        "          x = self.layer4(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "def train_mc_dropout(data, drop_prob, n_splits, num_epochs, num_units, batch_size, learn_rate, weight_decay, log_every, num_samples):\n",
        "    #torch.manual_seed(42)\n",
        "    kf = KFold(n_splits=n_splits)\n",
        "    in_dim = data.shape[1] - 1\n",
        "    train_logliks, test_logliks = [], []\n",
        "    train_rmses, test_rmses = [], []\n",
        "\n",
        "    for j, idx in enumerate(kf.split(data)):\n",
        "        num = j+1\n",
        "        print('SPLIT %d:' % num)\n",
        "        train_index, test_index = idx\n",
        "\n",
        "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
        "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
        "\n",
        "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
        "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
        "\n",
        "        x_train = (x_train - x_means)/x_stds\n",
        "        y_train = (y_train - y_means)/y_stds\n",
        "\n",
        "        x_test = (x_test - x_means)/x_stds\n",
        "        y_test = (y_test - y_means)/y_stds\n",
        "\n",
        "        net = MC_Dropout_Wrapper(network=MC_Dropout_Model_UCI(input_dim=in_dim, output_dim=1, num_units=num_units, drop_prob=drop_prob),\n",
        "                                 learn_rate=learn_rate, batch_size=batch_size, weight_decay=weight_decay)\n",
        "\n",
        "        fit_loss_train = np.zeros(num_epochs)\n",
        "\n",
        "        for i in range(num_epochs):\n",
        "            loss = net.fit(x_train, y_train)\n",
        "                \n",
        "            if i % log_every == 0 or i == num_epochs - 1:\n",
        "                test_loss, rmse = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
        "                test_loss, rmse = test_loss.cpu().data.numpy(), rmse.cpu().data.numpy()\n",
        "\n",
        "                print('Epoch: %4d, Train loss: %6.3f Test loss: %6.3f RMSE: %.3f' %\n",
        "                      (i, loss.cpu().data.numpy()/len(x_train), test_loss/len(x_test), rmse*y_stds[0]))\n",
        "\n",
        "\n",
        "        train_loss, train_rmse = net.get_loss_and_rmse(x_train, y_train, num_samples=num_samples)\n",
        "        test_loss, test_rmse = net.get_loss_and_rmse(x_test, y_test, num_samples=num_samples)\n",
        "        \n",
        "        train_logliks.append((train_loss.cpu().data.numpy()/len(x_train) + np.log(y_stds)[0]))\n",
        "        test_logliks.append((test_loss.cpu().data.numpy()/len(x_test) + np.log(y_stds)[0]))\n",
        "\n",
        "        train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
        "        test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
        "\n",
        "\n",
        "    print('Train log. lik. = %6.3f +/- %6.3f' % (-np.array(train_logliks).mean(), np.array(train_logliks).var()**0.5))\n",
        "    print('Test  log. lik. = %6.3f +/- %6.3f' % (-np.array(test_logliks).mean(), np.array(test_logliks).var()**0.5))\n",
        "    print('Train RMSE      = %6.3f +/- %6.3f' % (np.array(train_rmses).mean(), np.array(train_rmses).var()**0.5))\n",
        "    print('Test  RMSE      = %6.3f +/- %6.3f' % (np.array(test_rmses).mean(), np.array(test_rmses).var()**0.5))\n",
        "    \n",
        "    rmses =  list(np.array(test_rmses).flatten())\n",
        "    print(\"Test LogLike for different folds: \", test_logliks)\n",
        "    print(\"Test RMSEs   for different folds: \", rmses)\n",
        "\n",
        "    metrics = {\"train_log_like_mean\": -np.array(train_logliks).mean(), \"train_log_like_var\": np.array(train_logliks).var()**0.5,\n",
        "               \"test_log_like_mean\": -np.array(test_logliks).mean(), \"test_log_like_var\": np.array(test_logliks).var()**0.5,\n",
        "               \"train_rmse_mean\": np.array(train_rmses).mean(), \"train_rmse_var\": np.array(train_rmses).var()**0.5,\n",
        "               \"test_rmse_mean\": np.array(test_rmses).mean(), \"test_rmse_var\":np.array(test_rmses).var()**0.5,\n",
        "               \"rmse_values\": list(np.array(test_rmses).flatten()),\n",
        "               \"loglik_values\": list(np.array(test_logliks).flatten()),\n",
        "               }\n",
        "\n",
        "    return net, metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bA0eMK-pQAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MC_Dropout_Wrapper:\n",
        "    def __init__(self, network, learn_rate, batch_size , weight_decay):\n",
        "        \n",
        "        self.learn_rate = learn_rate\n",
        "        self.batch_size = batch_size\n",
        "        \n",
        "        self.network = network\n",
        "        self.network.cuda()\n",
        "        \n",
        "        self.optimizer = torch.optim.SGD(self.network.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
        "        self.loss_func = log_gaussian_loss\n",
        "    \n",
        "    def fit(self, x, y):\n",
        "        #torch.manual_seed(42)\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        # reset gradient and total loss\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        output = self.network(x)\n",
        "        loss = self.loss_func(output[:, :1], y, output[:, 1:].exp(), 1)\n",
        "        \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def get_loss_and_rmse(self, x, y, num_samples):\n",
        "        x, y = to_variable(var=(x, y), cuda=True)\n",
        "        \n",
        "        means, stds = [], []\n",
        "        for i in range(num_samples):\n",
        "            output = self.network(x)\n",
        "            means.append(output[:, :1])\n",
        "            stds.append(output[:, 1:].exp())\n",
        "        \n",
        "        means, stds = torch.cat(means, dim=1), torch.cat(stds, dim=1)\n",
        "        mean = means.mean(dim=-1)[:, None]\n",
        "        std = ((means.var(dim=-1) + stds.mean(dim=-1)**2)**0.5)[:, None]\n",
        "        loss = self.loss_func(mean, y, std, 1)\n",
        "        \n",
        "        rmse = ((mean - y)**2).mean()**0.5\n",
        "\n",
        "        return loss.detach().cpu(), rmse.detach().cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpg9g-sCKcro",
        "colab_type": "text"
      },
      "source": [
        "## RUN THE EXPERIMENTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdZ5w8R-6tMx",
        "colab_type": "text"
      },
      "source": [
        "# Regression on UCI data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfItkE1tKfsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set up the access to drive - we'll be saving our logs there\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "# define paths for the different experiments\n",
        "mcd_path = \"/content/drive/My Drive/\"+\"new_mcd_logs.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SFwYAXW5g7xF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c2618f40-4b1e-4e35-b544-577feb1c3a49"
      },
      "source": [
        "logs = []\n",
        "### some params\n",
        "n_splits = 30\n",
        "n_epochs = 200\n",
        "hidden   = 100\n",
        "models, log_metrics = [], []\n",
        "\n",
        "dataset = [data1, data2, data3, data4, data5] # list of all the datasets\n",
        "dataset_names = [\"Boston\", \"Concrete\", \"Energy\", \"Wine\", \"Yacht\"]\n",
        "hiddens = [1,2,3]\n",
        "\n",
        "for i, data in enumerate(dataset):\n",
        "  dataset_name = dataset_names[i]\n",
        "  for h in hiddens:\n",
        "    # run the training\n",
        "    num_units = [hidden for i in range(h)]\n",
        "    model, metric = train_mc_dropout(data=data, drop_prob=0.1, num_epochs=n_epochs, \n",
        "                                     n_splits=n_splits, num_units=num_units, batch_size =32, learn_rate=1e-4,\n",
        "                                     weight_decay=1e-1/len(data)**0.5, num_samples=20, log_every=50)\n",
        "    models.append(model)\n",
        "    # record to file: \n",
        "    log = {\"dataset\": dataset_name,\n",
        "           \"loss\": \"mc_dropout\",\n",
        "           \"alpha\": \"dropout_prob_\"+str(0.1),\n",
        "           \"n_layers\": h, \n",
        "           \"constant_hidden_size\": hidden, \n",
        "           \"metrics\": metric}\n",
        "    logs.append(log)\n",
        "    with open(mcd_path, \"a\") as f:\n",
        "      f.write(str(log)+\"\\n\")\n",
        "      print(log)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SPLIT 1:\n",
            "Epoch:    0, Train loss:  1.419 Test loss:  1.184 RMSE: 5.419\n",
            "Epoch:   50, Train loss:  0.429 Test loss:  0.252 RMSE: 2.472\n",
            "Epoch:  100, Train loss:  0.264 Test loss:  0.183 RMSE: 2.596\n",
            "Epoch:  150, Train loss:  0.296 Test loss:  0.222 RMSE: 2.322\n",
            "Epoch:  199, Train loss:  0.199 Test loss:  0.445 RMSE: 3.932\n",
            "SPLIT 2:\n",
            "Epoch:    0, Train loss:  2.026 Test loss:  1.675 RMSE: 7.092\n",
            "Epoch:   50, Train loss:  0.853 Test loss:  1.913 RMSE: 8.264\n",
            "Epoch:  100, Train loss:  1.578 Test loss:  0.942 RMSE: 4.029\n",
            "Epoch:  150, Train loss:  0.243 Test loss:  0.099 RMSE: 1.880\n",
            "Epoch:  199, Train loss:  0.106 Test loss: -0.033 RMSE: 1.830\n",
            "SPLIT 3:\n",
            "Epoch:    0, Train loss:  1.308 Test loss:  1.083 RMSE: 5.191\n",
            "Epoch:   50, Train loss:  1.360 Test loss:  1.233 RMSE: 7.896\n",
            "Epoch:  100, Train loss:  0.360 Test loss:  0.412 RMSE: 3.163\n",
            "Epoch:  150, Train loss:  0.224 Test loss:  0.017 RMSE: 2.106\n",
            "Epoch:  199, Train loss:  0.171 Test loss:  0.052 RMSE: 2.261\n",
            "SPLIT 4:\n",
            "Epoch:    0, Train loss:  1.554 Test loss:  1.583 RMSE: 8.896\n",
            "Epoch:   50, Train loss:  0.983 Test loss:  0.991 RMSE: 5.910\n",
            "Epoch:  100, Train loss:  1.342 Test loss:  1.126 RMSE: 5.304\n",
            "Epoch:  150, Train loss:  0.677 Test loss:  2.295 RMSE: 7.627\n",
            "Epoch:  199, Train loss:  0.234 Test loss:  1.865 RMSE: 5.143\n",
            "SPLIT 5:\n",
            "Epoch:    0, Train loss:  1.772 Test loss:  1.692 RMSE: 9.589\n",
            "Epoch:   50, Train loss:  0.394 Test loss:  0.330 RMSE: 3.072\n",
            "Epoch:  100, Train loss:  0.378 Test loss:  0.187 RMSE: 2.363\n",
            "Epoch:  150, Train loss:  0.418 Test loss:  0.238 RMSE: 2.217\n",
            "Epoch:  199, Train loss:  0.129 Test loss:  0.051 RMSE: 2.148\n",
            "SPLIT 6:\n",
            "Epoch:    0, Train loss:  1.507 Test loss:  1.243 RMSE: 7.740\n",
            "Epoch:   50, Train loss:  0.452 Test loss:  0.714 RMSE: 4.078\n",
            "Epoch:  100, Train loss:  0.587 Test loss:  0.284 RMSE: 1.923\n",
            "Epoch:  150, Train loss:  0.183 Test loss: -0.090 RMSE: 2.104\n",
            "Epoch:  199, Train loss:  0.246 Test loss: -0.020 RMSE: 2.053\n",
            "SPLIT 7:\n",
            "Epoch:    0, Train loss:  1.558 Test loss:  1.499 RMSE: 9.294\n",
            "Epoch:   50, Train loss:  0.330 Test loss:  1.310 RMSE: 7.346\n",
            "Epoch:  100, Train loss:  0.757 Test loss:  0.963 RMSE: 7.274\n",
            "Epoch:  150, Train loss:  0.146 Test loss:  1.195 RMSE: 7.088\n",
            "Epoch:  199, Train loss:  0.166 Test loss:  1.145 RMSE: 7.024\n",
            "SPLIT 8:\n",
            "Epoch:    0, Train loss:  1.447 Test loss:  1.240 RMSE: 7.145\n",
            "Epoch:   50, Train loss:  1.050 Test loss:  0.565 RMSE: 3.200\n",
            "Epoch:  100, Train loss:  0.253 Test loss:  0.364 RMSE: 3.011\n",
            "Epoch:  150, Train loss:  0.236 Test loss:  0.352 RMSE: 2.872\n",
            "Epoch:  199, Train loss:  1.284 Test loss:  0.818 RMSE: 4.750\n",
            "SPLIT 9:\n",
            "Epoch:    0, Train loss:  1.756 Test loss:  1.708 RMSE: 11.957\n",
            "Epoch:   50, Train loss:  0.690 Test loss:  0.703 RMSE: 5.823\n",
            "Epoch:  100, Train loss:  0.310 Test loss:  0.903 RMSE: 5.807\n",
            "Epoch:  150, Train loss:  0.901 Test loss:  0.542 RMSE: 4.216\n",
            "Epoch:  199, Train loss:  0.208 Test loss:  0.242 RMSE: 3.695\n",
            "SPLIT 10:\n",
            "Epoch:    0, Train loss:  1.403 Test loss:  1.427 RMSE: 11.058\n",
            "Epoch:   50, Train loss:  0.403 Test loss:  1.593 RMSE: 5.364\n",
            "Epoch:  100, Train loss:  0.318 Test loss:  0.912 RMSE: 4.346\n",
            "Epoch:  150, Train loss:  0.656 Test loss:  0.730 RMSE: 4.488\n",
            "Epoch:  199, Train loss:  0.255 Test loss:  0.598 RMSE: 3.813\n",
            "SPLIT 11:\n",
            "Epoch:    0, Train loss:  1.570 Test loss:  1.685 RMSE: 12.921\n",
            "Epoch:   50, Train loss:  0.769 Test loss:  0.849 RMSE: 6.215\n",
            "Epoch:  100, Train loss:  0.265 Test loss:  0.558 RMSE: 5.654\n",
            "Epoch:  150, Train loss:  0.196 Test loss:  0.522 RMSE: 5.184\n",
            "Epoch:  199, Train loss:  0.793 Test loss:  1.124 RMSE: 6.264\n",
            "SPLIT 12:\n",
            "Epoch:    0, Train loss:  1.727 Test loss:  1.250 RMSE: 5.746\n",
            "Epoch:   50, Train loss:  0.598 Test loss:  1.458 RMSE: 5.833\n",
            "Epoch:  100, Train loss:  0.417 Test loss:  0.369 RMSE: 2.938\n",
            "Epoch:  150, Train loss:  0.307 Test loss:  0.362 RMSE: 3.134\n",
            "Epoch:  199, Train loss:  0.095 Test loss:  0.431 RMSE: 3.099\n",
            "SPLIT 13:\n",
            "Epoch:    0, Train loss:  1.515 Test loss:  1.451 RMSE: 9.613\n",
            "Epoch:   50, Train loss:  1.215 Test loss:  0.962 RMSE: 7.343\n",
            "Epoch:  100, Train loss:  1.547 Test loss:  1.004 RMSE: 7.752\n",
            "Epoch:  150, Train loss:  0.645 Test loss:  0.794 RMSE: 7.184\n",
            "Epoch:  199, Train loss:  0.166 Test loss:  0.773 RMSE: 6.700\n",
            "SPLIT 14:\n",
            "Epoch:    0, Train loss:  1.507 Test loss:  1.431 RMSE: 9.941\n",
            "Epoch:   50, Train loss:  1.362 Test loss:  1.068 RMSE: 5.373\n",
            "Epoch:  100, Train loss:  0.257 Test loss:  0.635 RMSE: 3.939\n",
            "Epoch:  150, Train loss:  0.317 Test loss:  0.444 RMSE: 3.501\n",
            "Epoch:  199, Train loss:  0.183 Test loss:  0.410 RMSE: 3.399\n",
            "SPLIT 15:\n",
            "Epoch:    0, Train loss:  1.538 Test loss:  1.230 RMSE: 5.987\n",
            "Epoch:   50, Train loss:  0.894 Test loss:  0.508 RMSE: 2.778\n",
            "Epoch:  100, Train loss:  0.310 Test loss:  0.101 RMSE: 2.439\n",
            "Epoch:  150, Train loss:  0.337 Test loss:  0.452 RMSE: 3.504\n",
            "Epoch:  199, Train loss:  0.163 Test loss:  0.071 RMSE: 2.736\n",
            "SPLIT 16:\n",
            "Epoch:    0, Train loss:  1.487 Test loss:  1.326 RMSE: 7.821\n",
            "Epoch:   50, Train loss:  1.104 Test loss:  0.985 RMSE: 6.253\n",
            "Epoch:  100, Train loss:  0.244 Test loss:  0.247 RMSE: 3.555\n",
            "Epoch:  150, Train loss:  0.392 Test loss:  0.331 RMSE: 3.408\n",
            "Epoch:  199, Train loss:  0.160 Test loss:  0.200 RMSE: 3.352\n",
            "SPLIT 17:\n",
            "Epoch:    0, Train loss:  1.810 Test loss:  1.479 RMSE: 5.841\n",
            "Epoch:   50, Train loss:  1.034 Test loss:  0.729 RMSE: 3.213\n",
            "Epoch:  100, Train loss:  1.231 Test loss:  1.099 RMSE: 5.513\n",
            "Epoch:  150, Train loss:  1.301 Test loss:  0.530 RMSE: 2.582\n",
            "Epoch:  199, Train loss:  0.163 Test loss:  0.064 RMSE: 2.733\n",
            "SPLIT 18:\n",
            "Epoch:    0, Train loss:  1.541 Test loss:  1.236 RMSE: 6.260\n",
            "Epoch:   50, Train loss:  0.406 Test loss:  0.480 RMSE: 4.162\n",
            "Epoch:  100, Train loss:  0.526 Test loss:  0.437 RMSE: 3.786\n",
            "Epoch:  150, Train loss:  0.253 Test loss:  0.550 RMSE: 4.209\n",
            "Epoch:  199, Train loss:  1.234 Test loss:  0.612 RMSE: 4.594\n",
            "SPLIT 19:\n",
            "Epoch:    0, Train loss:  1.452 Test loss:  1.270 RMSE: 8.878\n",
            "Epoch:   50, Train loss:  0.501 Test loss:  0.586 RMSE: 3.655\n",
            "Epoch:  100, Train loss:  0.419 Test loss:  0.502 RMSE: 3.474\n",
            "Epoch:  150, Train loss:  0.228 Test loss:  0.471 RMSE: 3.464\n",
            "Epoch:  199, Train loss:  0.150 Test loss:  0.577 RMSE: 3.757\n",
            "SPLIT 20:\n",
            "Epoch:    0, Train loss:  1.496 Test loss:  1.411 RMSE: 10.393\n",
            "Epoch:   50, Train loss:  0.930 Test loss:  0.727 RMSE: 7.160\n",
            "Epoch:  100, Train loss:  0.516 Test loss:  0.445 RMSE: 6.583\n",
            "Epoch:  150, Train loss:  0.227 Test loss:  0.161 RMSE: 6.179\n",
            "Epoch:  199, Train loss:  0.140 Test loss:  0.129 RMSE: 6.172\n",
            "SPLIT 21:\n",
            "Epoch:    0, Train loss:  1.791 Test loss:  1.560 RMSE: 8.656\n",
            "Epoch:   50, Train loss:  0.421 Test loss:  0.562 RMSE: 4.751\n",
            "Epoch:  100, Train loss:  0.479 Test loss:  0.321 RMSE: 4.146\n",
            "Epoch:  150, Train loss:  0.236 Test loss:  0.187 RMSE: 4.862\n",
            "Epoch:  199, Train loss:  0.124 Test loss:  0.038 RMSE: 4.067\n",
            "SPLIT 22:\n",
            "Epoch:    0, Train loss:  1.529 Test loss:  1.425 RMSE: 7.831\n",
            "Epoch:   50, Train loss:  1.122 Test loss:  0.840 RMSE: 4.075\n",
            "Epoch:  100, Train loss:  0.325 Test loss:  0.373 RMSE: 3.096\n",
            "Epoch:  150, Train loss:  0.437 Test loss:  0.436 RMSE: 3.142\n",
            "Epoch:  199, Train loss:  0.211 Test loss:  0.341 RMSE: 2.902\n",
            "SPLIT 23:\n",
            "Epoch:    0, Train loss:  1.469 Test loss:  1.150 RMSE: 5.612\n",
            "Epoch:   50, Train loss:  0.391 Test loss:  0.225 RMSE: 2.123\n",
            "Epoch:  100, Train loss:  0.378 Test loss:  0.958 RMSE: 4.698\n",
            "Epoch:  150, Train loss:  0.178 Test loss:  0.093 RMSE: 2.042\n",
            "Epoch:  199, Train loss:  0.150 Test loss:  0.133 RMSE: 2.031\n",
            "SPLIT 24:\n",
            "Epoch:    0, Train loss:  1.369 Test loss:  1.462 RMSE: 9.747\n",
            "Epoch:   50, Train loss:  0.384 Test loss:  0.225 RMSE: 2.483\n",
            "Epoch:  100, Train loss:  0.539 Test loss:  0.374 RMSE: 2.369\n",
            "Epoch:  150, Train loss:  0.202 Test loss:  0.150 RMSE: 2.455\n",
            "Epoch:  199, Train loss:  0.267 Test loss:  0.369 RMSE: 3.497\n",
            "SPLIT 25:\n",
            "Epoch:    0, Train loss:  1.486 Test loss:  1.064 RMSE: 5.058\n",
            "Epoch:   50, Train loss:  0.398 Test loss:  0.241 RMSE: 2.747\n",
            "Epoch:  100, Train loss:  0.231 Test loss:  0.131 RMSE: 2.417\n",
            "Epoch:  150, Train loss:  0.212 Test loss:  0.365 RMSE: 2.711\n",
            "Epoch:  199, Train loss:  0.166 Test loss:  0.182 RMSE: 2.496\n",
            "SPLIT 26:\n",
            "Epoch:    0, Train loss:  1.412 Test loss:  1.459 RMSE: 9.206\n",
            "Epoch:   50, Train loss:  0.407 Test loss:  0.611 RMSE: 5.233\n",
            "Epoch:  100, Train loss:  0.275 Test loss:  0.392 RMSE: 4.345\n",
            "Epoch:  150, Train loss:  0.247 Test loss:  0.254 RMSE: 3.696\n",
            "Epoch:  199, Train loss:  0.145 Test loss:  0.147 RMSE: 3.515\n",
            "SPLIT 27:\n",
            "Epoch:    0, Train loss:  1.691 Test loss:  1.456 RMSE: 6.167\n",
            "Epoch:   50, Train loss:  1.256 Test loss:  0.723 RMSE: 2.831\n",
            "Epoch:  100, Train loss:  0.270 Test loss:  0.266 RMSE: 3.038\n",
            "Epoch:  150, Train loss:  0.213 Test loss:  0.176 RMSE: 2.624\n",
            "Epoch:  199, Train loss:  0.188 Test loss:  0.209 RMSE: 2.619\n",
            "SPLIT 28:\n",
            "Epoch:    0, Train loss:  1.544 Test loss:  1.386 RMSE: 8.885\n",
            "Epoch:   50, Train loss:  0.917 Test loss:  0.466 RMSE: 2.708\n",
            "Epoch:  100, Train loss:  0.248 Test loss: -0.068 RMSE: 2.035\n",
            "Epoch:  150, Train loss:  0.621 Test loss:  0.275 RMSE: 2.369\n",
            "Epoch:  199, Train loss:  1.010 Test loss:  0.596 RMSE: 4.045\n",
            "SPLIT 29:\n",
            "Epoch:    0, Train loss:  1.979 Test loss:  1.903 RMSE: 7.424\n",
            "Epoch:   50, Train loss:  0.487 Test loss:  0.527 RMSE: 5.807\n",
            "Epoch:  100, Train loss:  0.620 Test loss:  0.541 RMSE: 5.761\n",
            "Epoch:  150, Train loss:  0.180 Test loss:  0.289 RMSE: 5.210\n",
            "Epoch:  199, Train loss:  1.088 Test loss:  0.706 RMSE: 8.190\n",
            "SPLIT 30:\n",
            "Epoch:    0, Train loss:  1.663 Test loss:  1.312 RMSE: 6.995\n",
            "Epoch:   50, Train loss:  0.872 Test loss:  1.223 RMSE: 6.196\n",
            "Epoch:  100, Train loss:  0.272 Test loss:  0.256 RMSE: 2.586\n",
            "Epoch:  150, Train loss:  0.179 Test loss:  0.242 RMSE: 2.583\n",
            "Epoch:  199, Train loss:  0.170 Test loss:  0.153 RMSE: 2.399\n",
            "Train log. lik. = -2.490 +/-  0.256\n",
            "Test  log. lik. = -2.629 +/-  0.389\n",
            "Train RMSE      =  3.882 +/-  0.661\n",
            "Test  RMSE      =  3.873 +/-  1.614\n",
            "Test LogLike for different folds:  [2.7053051970057083, 2.191712237848042, 2.2867846876651416, 3.9541796533150544, 2.2656027228719933, 2.221673088413662, 3.245694853723791, 3.0511592786802058, 2.4724874825477294, 2.7396550023651316, 3.2718234157666046, 2.6972389182445053, 2.9848782818818336, 2.6286732201740812, 2.277525768259773, 2.433005499428304, 2.303966906329118, 2.851514374545893, 2.8161034790202835, 2.4029134723330765, 2.244267877034649, 2.5539286626758324, 2.3519836465269615, 2.6007309954270346, 2.4515419710781816, 2.3311394662436795, 2.42013178213092, 2.8445527457462427, 2.9346109872449624, 2.335721835408526]\n",
            "Test RMSEs   for different folds:  [4.067392650733956, 1.8326997173602986, 2.3139056415659556, 5.175562094089396, 2.197852621939959, 2.2421570891102105, 6.977803338853705, 4.803617893112981, 3.9091567654379644, 3.722261020227629, 6.325600499707604, 3.1333857633364937, 6.696703988692379, 3.3122490243539304, 2.6538316510995035, 3.410586472849435, 2.7609245031027996, 4.645708015816541, 3.8177493445160393, 6.258344234835424, 4.215450082836752, 2.8941126755743207, 2.0631786405355896, 3.729018698526738, 2.4615947656797843, 3.4186722663597795, 2.5884748245100995, 4.077343835986136, 8.214293159630477, 2.2586928324181024]\n",
            "{'dataset': 'Boston', 'loss': 'mc_dropout', 'alpha': 'dropout_prob_0.1', 'n_layers': 1, 'constant_hidden_size': 100, 'metrics': {'train_log_like_mean': -2.4895855291625875, 'train_log_like_var': 0.25574419447796387, 'test_log_like_mean': -2.6290169169978976, 'test_log_like_var': 0.3886798571601174, 'train_rmse_mean': 3.882050622153549, 'train_rmse_var': 0.6606902696613445, 'test_rmse_mean': 3.8726108037599998, 'test_rmse_var': 1.6136435401332678, 'rmse_values': [4.067392650733956, 1.8326997173602986, 2.3139056415659556, 5.175562094089396, 2.197852621939959, 2.2421570891102105, 6.977803338853705, 4.803617893112981, 3.9091567654379644, 3.722261020227629, 6.325600499707604, 3.1333857633364937, 6.696703988692379, 3.3122490243539304, 2.6538316510995035, 3.410586472849435, 2.7609245031027996, 4.645708015816541, 3.8177493445160393, 6.258344234835424, 4.215450082836752, 2.8941126755743207, 2.0631786405355896, 3.729018698526738, 2.4615947656797843, 3.4186722663597795, 2.5884748245100995, 4.077343835986136, 8.214293159630477, 2.2586928324181024], 'loglik_values': [2.7053051970057083, 2.191712237848042, 2.2867846876651416, 3.9541796533150544, 2.2656027228719933, 2.221673088413662, 3.245694853723791, 3.0511592786802058, 2.4724874825477294, 2.7396550023651316, 3.2718234157666046, 2.6972389182445053, 2.9848782818818336, 2.6286732201740812, 2.277525768259773, 2.433005499428304, 2.303966906329118, 2.851514374545893, 2.8161034790202835, 2.4029134723330765, 2.244267877034649, 2.5539286626758324, 2.3519836465269615, 2.6007309954270346, 2.4515419710781816, 2.3311394662436795, 2.42013178213092, 2.8445527457462427, 2.9346109872449624, 2.335721835408526]}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAYnEbzNsBex",
        "colab_type": "text"
      },
      "source": [
        "# Regression on GP ground truth\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VgFkpyifu4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_uncertainty_mcd(net, x_train, y_train):\n",
        "  samples = []\n",
        "  noises = []\n",
        "  for i in range(1000):\n",
        "      preds = net.network.forward(torch.linspace(-5, 5, 200).cuda()).cpu().data.numpy()\n",
        "      samples.append(preds[:, 0])\n",
        "      noises.append(np.exp(preds[:, 1]))\n",
        "      \n",
        "  samples = np.array(samples)\n",
        "  noises = np.array(noises)\n",
        "  means = (samples.mean(axis = 0)).reshape(-1)\n",
        "\n",
        "  aleatoric = (noises**2).mean(axis = 0)**0.5\n",
        "  epistemic = (samples.var(axis = 0)**0.5).reshape(-1)\n",
        "  total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
        "\n",
        "\n",
        "  c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
        "      '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
        "\n",
        "  plt.figure(figsize = (6, 5))\n",
        "  plt.style.use('default')\n",
        "  plt.scatter(x_train, y_train, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n",
        "  plt.fill_between(np.linspace(-5, 5, 200), means + aleatoric, means + total_unc, color = c[0], alpha = 0.3, label = 'Epistemic + Aleatoric')\n",
        "  plt.fill_between(np.linspace(-5, 5, 200), means - total_unc, means - aleatoric, color = c[0], alpha = 0.3)\n",
        "  plt.fill_between(np.linspace(-5, 5, 200), means - aleatoric, means + aleatoric, color = c[4], alpha = 0.4, label = 'Aleatoric')\n",
        "  plt.plot(np.linspace(-5, 5, 200), means, color = 'black', linewidth = 1)\n",
        "  plt.xlim([-5, 5])\n",
        "  plt.ylim([-5, 7])\n",
        "  plt.xlabel('$x$', fontsize=10)\n",
        "  plt.title('MC dropout', fontsize=10)\n",
        "  plt.tick_params(labelsize=10)\n",
        "  plt.xticks(np.arange(-4, 5, 2))\n",
        "  plt.yticks(np.arange(-4, 7, 2))\n",
        "  plt.gca().set_yticklabels([])\n",
        "  plt.gca().yaxis.grid(alpha=0.3)\n",
        "  plt.gca().xaxis.grid(alpha=0.3)\n",
        "  plt.savefig('mc_dropout_hetero.pdf', bbox_inches = 'tight')\n",
        "\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_EmfiSnWAon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_uncertainty_3row_mcd(nets, x, y):\n",
        "  fig, ax = plt.subplots(1, 3, figsize=(15, 4))\n",
        "  fig.suptitle(\"MC Dropout h=1 | h=2 | h=3\")\n",
        "  for n, net in enumerate(nets):\n",
        "    samples, noises = [], []\n",
        "    for i in range(100):\n",
        "        preds = nets[n].network.forward(torch.linspace(-5, 5, 200).cuda()).cpu().data.numpy()\n",
        "        samples.append(preds[:, 0])\n",
        "        noises.append(preds[:, 1])\n",
        "\n",
        "    samples = np.array(samples)\n",
        "    means = (samples.mean(axis = 0)).reshape(-1)\n",
        "    noises = np.array(noises)\n",
        "\n",
        "    aleatoric = (noises**2).mean(axis = 0)**0.5\n",
        "    epistemic = samples.var(axis = 0)**0.5\n",
        "    total_unc = (aleatoric**2 + epistemic**2)**0.5\n",
        "\n",
        "    c = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
        "        '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
        "\n",
        "    ax[n].scatter(x_train, y_train, s = 10, marker = 'x', color = 'black', alpha = 0.5)\n",
        "    ax[n].fill_between(np.linspace(-5, 5, 200), means + aleatoric, means + total_unc, color = c[0], alpha = 0.3, label = 'Epistemic + Aleatoric')\n",
        "    ax[n].fill_between(np.linspace(-5, 5, 200), means - total_unc, means - aleatoric, color = c[0], alpha = 0.3)\n",
        "    ax[n].fill_between(np.linspace(-5, 5, 200), means - aleatoric, means + aleatoric, color = c[4], alpha = 0.4, label = 'Aleatoric')\n",
        "    ax[n].plot(np.linspace(-5, 5, 200), means, color = 'black', linewidth = 1)\n",
        "    ax[n].set_xlim([-5, 5])\n",
        "    ax[n].set_ylim([-5, 7])\n",
        "    ax[n].set_xlabel('$x$', fontsize=10)\n",
        "    ax[n].set_title(\"h = \"+str(n+1), fontsize=10)\n",
        "    ax[n].tick_params(labelsize=10)\n",
        "    ax[n].set_xticks(np.arange(-4, 5, 2))\n",
        "    ax[n].set_yticks(np.arange(-4, 7, 2))\n",
        "    plt.gca().set_yticklabels([])\n",
        "    ax[n].grid(alpha=0.3)\n",
        "\n",
        "  plt.savefig('mcd_hetero.pdf', bbox_inches = 'tight')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOde8_JRpm2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(2)\n",
        "no_points = 400\n",
        "lengthscale = 1\n",
        "variance = 1.0\n",
        "sig_noise = 0.3\n",
        "x = np.random.uniform(-3, 3, no_points)[:, None]\n",
        "x.sort(axis = 0)\n",
        "\n",
        "\n",
        "k = GPy.kern.RBF(input_dim=1, variance=variance)#, lengthscale=lengthscale)\n",
        "C = k.K(x, x) + np.eye(no_points)*(x + 2)**2*sig_noise**2\n",
        "\n",
        "y = np.random.multivariate_normal(np.zeros((no_points)), C)[:, None]\n",
        "y = (y - y.mean())\n",
        "x_train = x[75:325]\n",
        "y_train = y[75:325]\n",
        "\n",
        "num_epochs, batch_size = 2000, len(x_train)\n",
        "\n",
        "#h_nets = []\n",
        "for num_units in [[200], [200, 300], [200, 300, 200]]: \n",
        "  net = MC_Dropout_Wrapper(network=MC_Dropout_Model_UCI(input_dim=1, output_dim=1, num_units=num_units, drop_prob=0.5),\n",
        "                          learn_rate=1e-4, batch_size=batch_size, weight_decay=1e-2)\n",
        "\n",
        "  fit_loss_train = np.zeros(num_epochs)\n",
        "  best_net, best_loss = None, float('inf')\n",
        "  nets, losses = [], []\n",
        "\n",
        "  for i in range(num_epochs): # \n",
        "      \n",
        "      loss = net.fit(x_train, y_train)\n",
        "      \n",
        "      if i % 100 == 0:\n",
        "          print('Epoch: %4d, Train loss = %7.3f' % (i, loss.cpu().data.numpy()/batch_size))\n",
        "  #h_nets.append(copy.copy(net))\n",
        "\n",
        "  plot_uncertainty_mcd(net, x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9FfAbOD5m_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}